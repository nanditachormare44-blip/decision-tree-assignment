{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11gttLPl_8VjtCMvNZx-m9gCBeoKbC3IU","timestamp":1763817433596}],"authorship_tag":"ABX9TyM67Tk4JmZF7SkiDPfg+NsT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["***QUESTION 1: What is a decision tree ,and how does it work in the context of classification?***\n","\n","           :- A decision tree is flowchart like model that splits data repeatedly based on feature values to make predictions.In the context of classification ,it asks yes/no questions at each node and reaches a class label at the leaf .It works by choosing the best feature to split data into seperate classes as cleanly as possible ."],"metadata":{"id":"Jz4lkkMNQ7Hb"}},{"cell_type":"markdown","source":["***QUESTION 2:Explain the concepts of gini impurity and entropy as impurity measures .how do they impact the splits in a decision tree?***\n","\n","             :- Gini impurity and Entropy as impurity (how mixed the classes are in a subset).The Decision tree chooses the split that minimize impurity in the child nodes.\n","             GINI IMPURITY : Measures how often a random sample would be misclassified .Formulae:1- ˙(pi2 ).Lower+purer\n","             ENTROPY:Measures uncertainty /disorder.formula-: -˙(pi log 2pi).Lower=purer.\n","             both tells the tree which split reduces mixed classes the most.the algoritm picks the feature threshold that gives the biggest drop in gini or entropy .Gini is fater ,Entropy is slightly prefers balanced splits trees are usually very similar either way."],"metadata":{"id":"xy_krdPDST5E"}},{"cell_type":"markdown","source":["***QUESTION 3:What is the difference between Pre-Pruning and Post-Pruning in decision trees?give one practical advantage of using each?***\n","\n","           :-˚Pre-Purning stop the tree from growing early (using max_depth, min_samples_split,etc.)while building it .\n","           Advantage:Faster training,less memory ,avoids overfitting from the start.\n","           ˚Post-Pruning lets the tree grow fully ,then removes unimportant branches\n","           Advantage:More accurate because it sees the full tree frist and removes only truly useless parts."],"metadata":{"id":"qwtIF_bwZYOy"}},{"cell_type":"markdown","source":["***QUESTION 4:What is information gain in decision tree,and why is it important for choosing best split?***\n","\n","             :-Information gain is the reduction in entropy(or gini) after a split.\n","             Formulae=impurity(parent)-weighted impurity of children.\n","             It is important because the algoritm chooses the split with highest imformation gain-that split gives the purest child nodes and seperate classes the best."],"metadata":{"id":"hphgdo22b_CI"}},{"cell_type":"code","source":["#QUESTION 5:Write a python program to:\n","#Load the iris dataset\n","#Train a decision tree classifier uusing the gini criterion\n","#Print the model's accuracy and. feature importance\n","\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","x = iris.data\n","y = iris.target\n","\n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n","\n","clf = DecisionTreeClassifier(criterion='gini',random_state=42)\n","clf.fit(x_train,y_train)\n","\n","y_pred = clf.predict(x_test)\n","accuracy = accuracy_score(y_test,y_pred)\n","\n","print(f\"Accuracy :{accuracy:.4f}\")\n","print(\"\\nFeature Importance:\")\n","\n","for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4b8mTWEd_iG","executionInfo":{"status":"ok","timestamp":1763814394780,"user_tz":-330,"elapsed":2152,"user":{"displayName":"Nandita Chormare","userId":"05382310000916564985"}},"outputId":"3b26dd73-40ba-45d9-e3c4-2afee5905c44"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy :1.0000\n","\n","Feature Importance:\n","sepal length (cm): 0.0000\n","sepal width (cm): 0.0191\n","petal length (cm): 0.8933\n","petal width (cm): 0.0876\n"]}]},{"cell_type":"code","source":["#Write a python program to:\n","#load data set\n","#train a decision tree classifier with max_depth=3 and compare its accuracy to a full grown tree\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the iris dataset (reusing if already loaded in the kernel)\n","# If x, y, x_train, x_test, y_train, y_test are not already defined, uncomment these lines:\n","# iris = load_iris()\n","# x = iris.data\n","# y = iris.target\n","# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","# Ensure x, y, x_train, x_test, y_train, y_test are defined.\n","# If they are not from a previous run, define them now.\n","# This block checks if they are defined, if not, it loads them.\n","if 'x' not in locals() or 'y' not in locals():\n","    iris = load_iris()\n","    x = iris.data\n","    y = iris.target\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","elif 'x_train' not in locals() or 'x_test' not in locals() or 'y_train' not in locals() or 'y_test' not in locals():\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","print(\"\\n--- Comparing Decision Tree Classifiers ---\")\n","\n","# 1. Train a Decision Tree Classifier with max_depth=3\n","clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n","clf_depth3.fit(x_train, y_train)\n","y_pred_depth3 = clf_depth3.predict(x_test)\n","accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n","print(f\"Accuracy with max_depth=3: {accuracy_depth3:.4f}\")\n","\n","# 2. Train a full-grown Decision Tree Classifier (default: no max_depth limit)\n","clf_full = DecisionTreeClassifier(random_state=42)\n","clf_full.fit(x_train, y_train)\n","y_pred_full = clf_full.predict(x_test)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","print(f\"Accuracy of full-grown tree: {accuracy_full:.4f}\")\n","\n","print(\"\\nComparison:\")\n","if accuracy_depth3 > accuracy_full:\n","    print(\"The tree with max_depth=3 performed better.\")\n","elif accuracy_full > accuracy_depth3:\n","    print(\"The full-grown tree performed better.\")\n","else:\n","    print(\"Both trees performed equally well.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpYupygoiL5H","executionInfo":{"status":"ok","timestamp":1763814714211,"user_tz":-330,"elapsed":24,"user":{"displayName":"Nandita Chormare","userId":"05382310000916564985"}},"outputId":"b65a7714-5e00-40af-e12c-0b335ca56792"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Comparing Decision Tree Classifiers ---\n","Accuracy with max_depth=3: 1.0000\n","Accuracy of full-grown tree: 1.0000\n","\n","Comparison:\n","Both trees performed equally well.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3efa29ae","executionInfo":{"status":"ok","timestamp":1763815102949,"user_tz":-330,"elapsed":1876,"user":{"displayName":"Nandita Chormare","userId":"05382310000916564985"}},"outputId":"f94ec238-220e-4e86-cd2c-83bf48bee394"},"source":["#write a python program to:\n","#load the bouston housing dataset\n","#train a decision tree regressor\n","#print the mean squared error (MSE)and feature importance\n","\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Load the California Housing dataset (recommended alternative to Boston Housing)\n","housing = fetch_california_housing()\n","X = housing.data\n","y = housing.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize and train a Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","regressor.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = regressor.predict(X_test)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","\n","# Print Feature Importance\n","print(\"\\nFeature Importance:\")\n","for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE): 0.5280\n","\n","Feature Importance:\n","MedInc: 0.5235\n","HouseAge: 0.0521\n","AveRooms: 0.0494\n","AveBedrms: 0.0250\n","Population: 0.0322\n","AveOccup: 0.1390\n","Latitude: 0.0900\n","Longitude: 0.0888\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dd5f334c","executionInfo":{"status":"ok","timestamp":1763815375338,"user_tz":-330,"elapsed":2895,"user":{"displayName":"Nandita Chormare","userId":"05382310000916564985"}},"outputId":"8b877ff1-f6b6-405b-ea8a-ea993c65c506"},"source":["#write python program to load the iris data set, train a decision tree's max_depth and min_sample_split using gridsearchCV and print the best parameters and the resulting model accuracy\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the parameter grid for GridSearchCV\n","# max_depth: The maximum depth of the tree.\n","# min_samples_split: The minimum number of samples required to split an internal node.\n","param_grid = {\n","    'max_depth': [None, 3, 5, 7, 10],\n","    'min_samples_split': [2, 5, 10, 15, 20]\n","}\n","\n","# Initialize a Decision Tree Classifier\n","dtc = DecisionTreeClassifier(random_state=42)\n","\n","# Initialize GridSearchCV\n","# cv=5 means 5-fold cross-validation\n","grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# Fit GridSearchCV to the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters found\n","print(\"Best parameters found by GridSearchCV:\")\n","print(grid_search.best_params_)\n","\n","# Get the best estimator (model) from GridSearchCV\n","best_dtc = grid_search.best_estimator_\n","\n","# Make predictions on the test set using the best model\n","y_pred = best_dtc.predict(X_test)\n","\n","# Calculate and print the accuracy of the best model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nAccuracy of the best model on the test set: {accuracy:.4f}\")\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters found by GridSearchCV:\n","{'max_depth': None, 'min_samples_split': 10}\n","\n","Accuracy of the best model on the test set: 1.0000\n"]}]},{"cell_type":"markdown","source":["***QUESTION:Imagine you're working as a data scientist for a health care company that wants to predict whether a patient has certain diease .you have alarge dataset with mixed data types and some missing values 1.handel the missing value 2.encode the categorical features 3.train a decision tree model 4.tune it's pyperparameters 5.evaluates the performance  and describe what bussiness value this model could provide in the real world setting.***\n","\n","            :-1.Handling missing values:use simplelmputer or KNNImputer if data is complex.drop coloums /rows only if >50-60% values are missing.\n","\n","             2.Encode categoricalfeatures: apply one-hot encoding for nominal categories(eg.gender,blood type)and ordinal/target encoding if many categories exist.descision trees can't handel text labels directly.\n","\n","             3.Train a decision tree:start with decision tree classifier using gini criterion .split data into train/validation/test .\n","\n","             4.Tune it's hyperparameters:use gridsearchCV or randomizedsearchCV to tune:max_depth,min_sample_split,min_sample_leaf,max_features\n","             this prevents overfitting and improves generalization.\n","\n","             5.Evaluate it's performance:use accuracy ,precision ,recall,F1-score,and especiallly AUC-ROC (critical in healthcare),check confusion matrix to minimize false negative.\n","\n","             6.Bussiness value in real world setting :earlyand accurate disease prediction helps doctors prioritize high risk patients,reduce unnecessary tests,enables preventive care and better resource allocation,Interpretable model allows doctors to understand and trust why a paitient is flagged as high risk."],"metadata":{"id":"IuoARyDzoKSz"}}]}